{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S51fzpruzOtV"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DuHebHK8zOtZ"
      },
      "outputs": [],
      "source": [
        "NAME = \"ANTONIS PRODROMOU\"\n",
        "ID = \"238\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj_WB-MDzOtZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqunN9lFKCSQ"
      },
      "source": [
        "# Introduction & Learning Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99XRNOtTKFmw"
      },
      "source": [
        "Welcome to your second assignment! The goal is to get hands-on experience with two fundamental ways of modeling language: i) N-gram Language Models (LMs) that capture the probability of a sequence of words, ii) Word Embeddings: Using dense vectors (fasttext) to represent word meaning.\n",
        "\n",
        "You will first build two separate n-gram LMs—one for positive reviews and one for negative—and use perplexity to see how well they model new sentences. Then, you will use word embeddings to build a sentiment classifier.\n",
        "\n",
        "This assignment will test your understanding of the concepts from the Speech and Language Processing book, Chapters 3 (N-gram Language Models) and 5 (Vector Semantics and Embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-6uHa5CrJ06f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anton/Desktop/Data Science/MSc Data and Web Science/DWS104 Επεξεργασία Φυσικής Γλώσσας/Exercises/Exercise2/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt_tab to /Users/anton/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/anton/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading IMDB dataset from Hugging Face...\n",
            "Loaded 25000 training examples and 25000 test examples.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.util import pad_sequence, ngrams\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
        "from nltk.lm import Laplace, MLE\n",
        "import re\n",
        "import requests, zipfile, io\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the IMDB dataset from Hugging Face\n",
        "print(\"Loading IMDB dataset from Hugging Face...\")\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# The dataset is a DatasetDict. We'll convert the 'train' and 'test' splits to pandas\n",
        "X_train_df = dataset['train'].to_pandas()\n",
        "X_test_df = dataset['test'].to_pandas()\n",
        "\n",
        "print(f\"Loaded {len(X_train_df)} training examples and {len(X_test_df)} test examples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GucAvQrgPaG5"
      },
      "source": [
        "# Part 1 - N-gram Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cui-xFNhP-gB"
      },
      "source": [
        "In this section, you will use nltk to build, train, and evaluate n-gram language models. You will create two separate models, one  trained only on positive reviews, and one trained only on negative reviews.\n",
        "\n",
        "You will then use perplexity to measure how \"surprising\" a new sentence is to each model. A lower perplexity score means the model finds the sentence more probable (i.e., it \"fits\" the model better).\n",
        "\n",
        "We will use trigrams (n=3) and Laplace (Add-1) smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5hdkQL-Pmzy"
      },
      "source": [
        "## Task 1.1 Train an N-gram LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJMSzyNKPrTK"
      },
      "source": [
        "Create a function train_lm that takes a list of tokenized sentences (a list of lists of tokens) and returns a trained 3-gram nltk language model with Laplace smoothing (nltk.lm.Laplace). Use the padded_everygram_pipeline function to process the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "id": "ydCWymQlPj45",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b32651a507d63e7d42fa98477d88b3cd",
          "grade": false,
          "grade_id": "cell-aa576ca266e95f5e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# the trigram model means that we are considering the two previous words\n",
        "# Laplace smoothing: add one to all the n-gram counts before we normalize them into probabilities\n",
        "\n",
        "def train_lm(sentences):\n",
        "    N = 3\n",
        "    # padded_everygram_pipeline returns iterators\n",
        "    train, vocab = padded_everygram_pipeline(N, sentences)\n",
        "    # Laplace() creates a new LanguageModel, implementing Laplace (add one) smoothing\n",
        "    lm = Laplace(N)\n",
        "    # I use vocab to initialize counts for unseen n-grams\n",
        "    lm.fit(train, vocab)\n",
        "    return lm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ZSVzHf23Qy5b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c494d140b8a15cdbc826382118c2d302",
          "grade": true,
          "grade_id": "cell-76d82dc3e9d4cf91",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing data for training N-gram LMs...\n",
            "Total positive sentences for LM: 130829\n",
            "Total negative sentences for LM: 137359\n",
            "Example positive sentence: ['zentropa', 'has', 'much', 'in', 'common', 'with', 'the', 'third', 'man', ',', 'another', 'noir-like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'europe', '.']\n",
            "Example negative sentence: ['i', 'rented', 'i', 'am', 'curious-yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u.s.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '``', 'controversial', \"''\", 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.']\n",
            "Training positive language model\n",
            "Training negative language model\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "# Convert the train split of the DatasetDict to pandas\n",
        "X_train_df = dataset['train'].to_pandas()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    return [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "print(\"Preparing data for training N-gram LMs...\")\n",
        "# Get all positive and negative reviews and create a list of sentences, each\n",
        "# with a list of tokens suitable for nltk LM training\n",
        "pos_sents = [sent for review in X_train_df[X_train_df['label'] == 1]['text'] for sent in preprocess(review)]\n",
        "neg_sents = [sent for review in X_train_df[X_train_df['label'] == 0]['text'] for sent in preprocess(review)]\n",
        "print(f\"Total positive sentences for LM: {len(pos_sents)}\")\n",
        "print(f\"Total negative sentences for LM: {len(neg_sents)}\")\n",
        "print(f\"Example positive sentence: {pos_sents[0]}\")\n",
        "print(f\"Example negative sentence: {neg_sents[0]}\")\n",
        "\n",
        "print(\"Training positive language model\")\n",
        "pos_lm = train_lm(pos_sents)\n",
        "print(\"Training negative language model\")\n",
        "neg_lm = train_lm(neg_sents)\n",
        "\n",
        "assert pos_lm.counts[['great']]['movie'] == 294\n",
        "assert neg_lm.counts[['bad']]['movie'] == 320\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIo5XM6ZQNLA"
      },
      "source": [
        "## Task 1.2 Calculate Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQF3soPsQQxB"
      },
      "source": [
        "Create a calculate perplexity function that takes a trained model and an untokenized sentence and computes its perplexity. You can use the perplexity function of nltk models. Remember to apply the same preprocessing as during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "id": "LveALBEwQSw6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4dffc9f8475d185cad5d4e1f146cead8",
          "grade": false,
          "grade_id": "cell-ad879e5adedc3219",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(lm, sentence):\n",
        "    N = 3\n",
        "    # preprocess returns a list of lists\n",
        "    tokenized_sents = preprocess(sentence)\n",
        "\n",
        "    # padded_everygram_pipeline creates two iterators: an iterator of padded n-grams\n",
        "    # and an iterator of padded words for vocabulary training\n",
        "    test_data, vocabulary = padded_everygram_pipeline(N, tokenized_sents)\n",
        "\n",
        "    # flatten the test_data iterator into a single list of n-grams\n",
        "    flattened = []\n",
        "    # loop over sentences\n",
        "    for sent_ngrams in test_data:\n",
        "        # loop over the n-grams in that sentence\n",
        "        for ngram in sent_ngrams:\n",
        "            flattened.append(ngram)\n",
        "    test_data = flattened\n",
        "\n",
        "    # Calculate perplexity using the model's perplexity method\n",
        "    return lm.perplexity(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "r5L7XQS9mqij",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "48a465cb45f0188eb1494bbca2deb351",
          "grade": true,
          "grade_id": "cell-4c27de09fd8ad6e2",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "test_pos_sent = \"This was a truly great and wonderful film.\"\n",
        "per_pos_pos = calculate_perplexity(pos_lm, test_pos_sent)\n",
        "per_neg_pos = calculate_perplexity(neg_lm, test_pos_sent)\n",
        "assert per_pos_pos < per_neg_pos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfidF2wEp6-d"
      },
      "source": [
        "# Part 2 - Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeORn4JUp_2U"
      },
      "source": [
        "In this section, we'll switch gears. Instead of n-grams, we'll represent text using pre-trained fasttext embeddings. We will use a \"sentence embedding\" technique by averaging the word vectors for all words in a sentence to build a sentiment classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nV62Zo9wqSNT"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
        "# model = fasttext.load_model(model_path)\n",
        "model = fasttext.load_model(\"data/model.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TH4VYBrqLgK"
      },
      "source": [
        "## Task 2.1 Implement Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB6vVZcDqdGs"
      },
      "source": [
        "To understand embeddings, let's look at their properties. A key operation is cosine similarity, which measures the similarity between two vectors. Implement the function below.Hint: The formula is $similarity = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "id": "94pVwUBLqQwj",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3606b69bd417ed8b1fb58c785e96a5e8",
          "grade": false,
          "grade_id": "cell-75dae2672c2be221",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "    vctr_dot_product = vec_a.dot(vec_b)\n",
        "    vector_magnitude_product = np.linalg.norm(vec_a) * np.linalg.norm(vec_b)\n",
        "    if vector_magnitude_product!= 0:\n",
        "        vector_similiarity = vctr_dot_product / vector_magnitude_product\n",
        "    else:\n",
        "        vector_similiarity = 0\n",
        "    return vector_similiarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bECQBbYrqhVr",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b51cd02a4246bb35f4d9579e15059d02",
          "grade": true,
          "grade_id": "cell-9fa2227fd77a189f",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "vec_good = model['good']\n",
        "vec_nice = model['nice']\n",
        "vec_king = model['king']\n",
        "assert cosine_similarity(vec_good, vec_nice) > cosine_similarity(vec_good, vec_king)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ybkL9nqv_O7"
      },
      "source": [
        "## Task 2.2 Document Vector Averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbZn_Y6gwDBq"
      },
      "source": [
        "To create a sentiment classifier, let's first implement a method that converts a document into the average of the embeddings of the words inside the document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "id": "K1VwQKJlwDJr",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ab116ef89c2c4afcaa78037a114e07c7",
          "grade": false,
          "grade_id": "cell-fd8d2a971686f73e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def average_document_vector(doc_text, embeddings_dict):\n",
        "    words = doc_text.split()\n",
        "    total_embeddings = []\n",
        "    for word in words:\n",
        "        total_embeddings.append(embeddings_dict[word])\n",
        "    if total_embeddings:\n",
        "        return np.mean(np.array(total_embeddings), axis=0)\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "g21B48znwd3U",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7babdd62c7d0fa06c8b0b97059fdcc89",
          "grade": true,
          "grade_id": "cell-790754c91009bde9",
          "locked": true,
          "points": 2.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting 1000 documents to averaged fasttext vectors...\n",
            "Feature matrix shape (X): (1000, 300)\n",
            "Label vector shape (y): (1000,)\n",
            "Training Logistic Regression model...\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "positive_samples = train_dataset.filter(lambda x: x[\"label\"] == 1)\n",
        "negative_samples = train_dataset.filter(lambda x: x[\"label\"] == 0)\n",
        "n = 500\n",
        "positive_subsample = positive_samples.select(range(n))\n",
        "negative_subsample = negative_samples.select(range(n))\n",
        "subsampled_dataset = concatenate_datasets([positive_subsample, negative_subsample])\n",
        "\n",
        "texts = subsampled_dataset['text']\n",
        "labels = subsampled_dataset['label']\n",
        "\n",
        "# 2. Convert text documents to averaged vector features\n",
        "print(f\"Converting {len(texts)} documents to averaged fasttext vectors...\")\n",
        "X = np.array([average_document_vector(text, model) for text in texts])\n",
        "y = np.array(labels)\n",
        "\n",
        "# Check the shape of the features\n",
        "print(f\"Feature matrix shape (X): {X.shape}\")\n",
        "print(f\"Label vector shape (y): {y.shape}\")\n",
        "\n",
        "# 3. Split data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(\n",
        "#    X, y, test_size=0.2, random_state=42, stratify=y\n",
        "#)\n",
        "#print(f\"Train/Test split: {len(X_train)} training samples, {len(X_test)} testing samples.\")\n",
        "\n",
        "# 4. Train the Logistic Regression Model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "classifier.fit(X, y)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "assert classifier.predict([average_document_vector(\"This was a truly great and wonderful film.\", model)])[0] == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
