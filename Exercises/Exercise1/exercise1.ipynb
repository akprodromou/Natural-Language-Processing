{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywOity7GC8mL"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DwePKnzPC8mT"
      },
      "outputs": [],
      "source": [
        "NAME = \"ANTONIS PRODROMOU\"\n",
        "ID = \"238\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0piYd_1zC8mX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MhQwAZwC8mY"
      },
      "source": [
        "### Part A: Regular expressions (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i8cs2xZC8mZ"
      },
      "source": [
        "In the exercised of this part you should NOT assume that gutenberg files are locally available in the file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mS_INp_2C8mb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /Users/anton/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /Users/anton/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SxEJhyjC8mc"
      },
      "source": [
        "Complete the following function using the search function of the re library to find and return the year that a file in the gutenberg collection was written. Return the year as string or the empty string if not found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Paradise Lost by John Milton 1667] \n",
            " \n",
            " \n",
            "Book I \n",
            " \n",
            " \n",
            "Of Man's first disobedience, and the fruit \n",
            "Of that forbidden tree whose mortal taste \n",
            "Brought death into the World, and all our woe, \n",
            "With loss of Eden, till one greater Man \n",
            "Restore us, and regain the blissful seat, \n",
            "Sing, Heavenly Muse, that, o\n"
          ]
        }
      ],
      "source": [
        "raw = gutenberg.raw('milton-paradise.txt')\n",
        "print(raw[0:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_year(file):\n",
        "    raw = gutenberg.raw(file)\n",
        "    # get the the first match only\n",
        "    title = re.search('\\[.*by.*[0-9]{4}.*]', raw)\n",
        "    if title:\n",
        "        date = re.search('[0-9]{4}', title.group())\n",
        "        return date.group()\n",
        "    else:\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "assert get_year('carroll-alice.txt') == '1865'\n",
        "assert get_year('shakespeare-hamlet.txt') == '1599'\n",
        "assert get_year('milton-paradise.txt') == '1667'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbBo7TvpC8mf"
      },
      "source": [
        "Complete the following function using the findall function of the re library to compute and report the number of times some text appears in a file of the gutenberg collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "id": "UYIJ74lhC8mg",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aa33bf40ffcefca144136b19837cd66f",
          "grade": false,
          "grade_id": "cell-fcd8fa5f85a0dbd7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def find_mentions(text, file):\n",
        "    raw = gutenberg.raw(file)\n",
        "    # return all non-overlapping matches of pattern in string as a list of strings or tuples\n",
        "    # re.escape makes sure that special characters in the text are escaped\n",
        "    matches = re.findall(re.escape(text), raw)\n",
        "    return len(matches)\n",
        "\n",
        "find_mentions('CHAPTER ', 'carroll-alice.txt') == 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "VwsrqAYcC8mh",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2b18a2376ce1fcada625b98f0299ca25",
          "grade": true,
          "grade_id": "cell-b5c350fd93ad1d75",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "assert find_mentions('CHAPTER ', 'carroll-alice.txt') == 12\n",
        "assert find_mentions('UNK', 'shakespeare-caesar.txt') == 0\n",
        "assert find_mentions('Caesar', 'shakespeare-caesar.txt') == 227"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHVjqtqcC8mi"
      },
      "source": [
        "Complete the following function to return the mean number of words in the sentences of a gutenberg file that is given as input. The number should be rounded to 2 decimals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7752\n",
            "192427\n",
            "24.82\n"
          ]
        }
      ],
      "source": [
        "# get the sentences\n",
        "sents = gutenberg.sents('austen-emma.txt')\n",
        "sents_length = len(sents)\n",
        "print(sents_length)\n",
        "\n",
        "words = gutenberg.words('austen-emma.txt')\n",
        "words_length = len(words)\n",
        "print(words_length)\n",
        "\n",
        "words_mean = round(words_length / sents_length, 2)\n",
        "print(words_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "id": "gFBe_LBSC8mj",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "23ded46ba3aec1e38b55235406faed9a",
          "grade": false,
          "grade_id": "cell-4bc64ff6404ecf22",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.94"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def average_sentence_length(file):\n",
        "    sents = gutenberg.sents(file)\n",
        "    sents_length = len(sents)\n",
        "    words = gutenberg.words(file)\n",
        "    words_length = len(words)\n",
        "    words_mean = round(words_length / sents_length, 2)\n",
        "    return words_mean\n",
        "\n",
        "average_sentence_length('shakespeare-caesar.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Fq-HL1otC8mj",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a6c2caf2afa8886e41197a836b7b1a4f",
          "grade": true,
          "grade_id": "cell-27d42c9e86ec9169",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "assert average_sentence_length('shakespeare-caesar.txt') == 11.94\n",
        "assert average_sentence_length('austen-emma.txt') == 24.82"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M34_AVPC8mk"
      },
      "source": [
        "## Part B: Byte-Pair Encoding (7 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvvUhJHEC8ml"
      },
      "source": [
        "Implement a function that returns the vocabulary learned by the byte-pair encoding algorithm given a corpus, after a number of iterations. The characters in the corpus, sorted in alphabetical order, will be the first members of the vocabulary, to be extended with as many tokens as the number of iterations given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "GwhrUe4uC8ml",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "94cdfd63518f5ebdeca163e4bf28505f",
          "grade": false,
          "grade_id": "cell-00f295323218bcab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus C is ['set', ' new', ' new', ' renew', ' reset', ' renew']\n",
            "Word frequences is {'set': 1, ' new': 2, ' renew': 2, ' reset': 1}\n",
            "{'set': ['s', 'e', 't'], ' new': ['_', 'n', 'e', 'w'], ' renew': ['_', 'r', 'e', 'n', 'e', 'w'], ' reset': ['_', 'r', 'e', 's', 'e', 't']}\n",
            "Initial vocabulary is: ['_', 'e', 'n', 'r', 's', 't', 'w']\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'n'), 2), (('n', 'e'), 4), (('e', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'n'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('n', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'ne'), 2), (('ne', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'ne'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('ne', 'w')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "\n",
            "final vocabulary after all merges:\n",
            "['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new']\n"
          ]
        }
      ],
      "source": [
        "corpus = \"set new new renew reset renew\"\n",
        "\n",
        "\n",
        "def bpe_train(corpus, iterations):\n",
        "    # Create the corpus dictionary\n",
        "    # First, we'll break up the corpus into words, with leading whitespace, together with\n",
        "    # their counts; no merges will be allowed to go beyond these word boundaries\n",
        "    # add any unique whitespace characters\n",
        "    words = re.findall(r\"\\b\\s*\\S+\\b\", corpus)\n",
        "    print(f\"Corpus C is {words}\")\n",
        "\n",
        "    # get word frequencies\n",
        "    word_count_dict = {}\n",
        "    for word in words:\n",
        "        try:\n",
        "            word_count_dict[word] += 1\n",
        "        except:\n",
        "            word_count_dict[word] = 1\n",
        "    print(f\"Word frequences is {word_count_dict}\")\n",
        "\n",
        "    # This dict will store the symbols each word contains\n",
        "    word_tokens = {}\n",
        "    # unpack words and their frequency\n",
        "    for word, freq in word_count_dict.items():\n",
        "        # From the expected results, I need to replace whitespaces with an underscore for each word\n",
        "        word_with_underscore = word.replace(\" \", \"_\")\n",
        "        # Split each word into individual characters and store in the dict\n",
        "        word_tokens[word] = list(word_with_underscore)\n",
        "\n",
        "    print(word_tokens)\n",
        "\n",
        "    vocabulary = set()\n",
        "    # Initial vocabulary\n",
        "    for token in word_tokens.values():\n",
        "        vocabulary.update(token)\n",
        "    # sort it in alphabetical order as the exercise requires\n",
        "    vocabulary = sorted(list(vocabulary))\n",
        "    print(f\"Initial vocabulary is: {vocabulary}\")\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # this dict will store all the pairs of the iteration\n",
        "        pairs = {}\n",
        "        # unpack similar to before\n",
        "        for word, freq in word_count_dict.items():\n",
        "            # get a list of symbols for that word\n",
        "            symbols = word_tokens[word]\n",
        "            # -1 to avoid error for the last symbol\n",
        "            for j in range(len(symbols) - 1):\n",
        "                # store them in a tuple\n",
        "                pair = (symbols[j], symbols[j + 1])\n",
        "                # get(): give me the current count for this pair if it exists, otherwise return 0\n",
        "                pairs[pair] = pairs.get(pair, 0) + freq\n",
        "\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # show the pair frequencies\n",
        "        print(\"Pair frequencies:\")\n",
        "        print(pairs.items())\n",
        "\n",
        "        # most_frequent_pair = max(pairs, key=pairs.get)\n",
        "        most_frequent_pair_info = sorted(pairs.items(), key=lambda x:x[1], reverse=True)[0]\n",
        "        most_frequent_pair = most_frequent_pair_info[0]\n",
        "        print(f\"Most frequent pair to merge: {most_frequent_pair}\")\n",
        "\n",
        "        # merge the pair in every word\n",
        "        new_word_tokens = {}\n",
        "        for word, symbols in word_tokens.items():\n",
        "            new_symbols = []\n",
        "            j = 0\n",
        "            while j < len(symbols):\n",
        "                # if this position starts the most frequent pair, merge them\n",
        "                if j < len(symbols) - 1 and (symbols[j], symbols[j + 1]) == most_frequent_pair:\n",
        "                    new_symbols.append(symbols[j] + symbols[j + 1])\n",
        "                    # skip the next symbol because it's merged\n",
        "                    j += 2\n",
        "                else:\n",
        "                    new_symbols.append(symbols[j])\n",
        "                    j += 1\n",
        "            new_word_tokens[word] = new_symbols\n",
        "\n",
        "        # update for next iteration\n",
        "        word_tokens = new_word_tokens\n",
        "        vocabulary.append(most_frequent_pair[0] + most_frequent_pair[1])\n",
        "\n",
        "        # show how each word looks after the merge\n",
        "        print(\"Updated tokens per word:\")\n",
        "        for word, tokens in word_tokens.items():\n",
        "            print(word_tokens.items())\n",
        "\n",
        "    print(\"\\nfinal vocabulary after all merges:\")\n",
        "    print(list(vocabulary))\n",
        "    return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XOEuIxBFC8mm",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f616a98cd2306268b66028c0c8476486",
          "grade": true,
          "grade_id": "cell-9be88b50d1385e55",
          "locked": true,
          "points": 7,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus C is ['set', ' new', ' new', ' renew', ' reset', ' renew']\n",
            "Word frequences is {'set': 1, ' new': 2, ' renew': 2, ' reset': 1}\n",
            "{'set': ['s', 'e', 't'], ' new': ['_', 'n', 'e', 'w'], ' renew': ['_', 'r', 'e', 'n', 'e', 'w'], ' reset': ['_', 'r', 'e', 's', 'e', 't']}\n",
            "Initial vocabulary is: ['_', 'e', 'n', 'r', 's', 't', 'w']\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'n'), 2), (('n', 'e'), 4), (('e', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'n'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('n', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'ne'), 2), (('ne', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'ne'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('ne', 'w')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'new'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('_', 'r')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_r', 'e'), 3), (('e', 'new'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('_r', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "\n",
            "final vocabulary after all merges:\n",
            "['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new', '_r', '_re']\n",
            "Corpus C is ['set', ' new', ' new', ' renew', ' reset', ' renew']\n",
            "Word frequences is {'set': 1, ' new': 2, ' renew': 2, ' reset': 1}\n",
            "{'set': ['s', 'e', 't'], ' new': ['_', 'n', 'e', 'w'], ' renew': ['_', 'r', 'e', 'n', 'e', 'w'], ' reset': ['_', 'r', 'e', 's', 'e', 't']}\n",
            "Initial vocabulary is: ['_', 'e', 'n', 'r', 's', 't', 'w']\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'n'), 2), (('n', 'e'), 4), (('e', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'n'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('n', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'ne'), 2), (('ne', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'ne'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('ne', 'w')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'new'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('_', 'r')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_r', 'e'), 3), (('e', 'new'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('_r', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_re', 'new'), 2), (('_re', 's'), 1)])\n",
            "Most frequent pair to merge: ('s', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('se', 't'), 2), (('_', 'new'), 2), (('_re', 'new'), 2), (('_re', 'se'), 1)])\n",
            "Most frequent pair to merge: ('se', 't')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('_', 'new'), 2), (('_re', 'new'), 2), (('_re', 'set'), 1)])\n",
            "Most frequent pair to merge: ('_', 'new')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('_re', 'new'), 2), (('_re', 'set'), 1)])\n",
            "Most frequent pair to merge: ('_re', 'new')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "\n",
            "final vocabulary after all merges:\n",
            "['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new', '_r', '_re', 'se', 'set', '_new', '_renew']\n",
            "Corpus C is ['set', ' new', ' new', ' renew', ' reset', ' renew']\n",
            "Word frequences is {'set': 1, ' new': 2, ' renew': 2, ' reset': 1}\n",
            "{'set': ['s', 'e', 't'], ' new': ['_', 'n', 'e', 'w'], ' renew': ['_', 'r', 'e', 'n', 'e', 'w'], ' reset': ['_', 'r', 'e', 's', 'e', 't']}\n",
            "Initial vocabulary is: ['_', 'e', 'n', 'r', 's', 't', 'w']\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'n'), 2), (('n', 'e'), 4), (('e', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'n'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('n', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'ne', 'w']), (' renew', ['_', 'r', 'e', 'ne', 'w']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'ne'), 2), (('ne', 'w'), 4), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'ne'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('ne', 'w')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_', 'r', 'e', 'new']), (' reset', ['_', 'r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_', 'r'), 3), (('r', 'e'), 3), (('e', 'new'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('_', 'r')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_r', 'e', 'new']), (' reset', ['_r', 'e', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_r', 'e'), 3), (('e', 'new'), 2), (('e', 's'), 1)])\n",
            "Most frequent pair to merge: ('_r', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "dict_items([('set', ['s', 'e', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 's', 'e', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('s', 'e'), 2), (('e', 't'), 2), (('_', 'new'), 2), (('_re', 'new'), 2), (('_re', 's'), 1)])\n",
            "Most frequent pair to merge: ('s', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "dict_items([('set', ['se', 't']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'se', 't'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('se', 't'), 2), (('_', 'new'), 2), (('_re', 'new'), 2), (('_re', 'se'), 1)])\n",
            "Most frequent pair to merge: ('se', 't')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_', 'new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('_', 'new'), 2), (('_re', 'new'), 2), (('_re', 'set'), 1)])\n",
            "Most frequent pair to merge: ('_', 'new')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_re', 'new']), (' reset', ['_re', 'set'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('_re', 'new'), 2), (('_re', 'set'), 1)])\n",
            "Most frequent pair to merge: ('_re', 'new')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_re', 'set'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('_re', 'set'), 1)])\n",
            "Most frequent pair to merge: ('_re', 'set')\n",
            "Updated tokens per word:\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_reset'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_reset'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_reset'])])\n",
            "dict_items([('set', ['set']), (' new', ['_new']), (' renew', ['_renew']), (' reset', ['_reset'])])\n",
            "\n",
            "final vocabulary after all merges:\n",
            "['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new', '_r', '_re', 'se', 'set', '_new', '_renew', '_reset']\n",
            "Corpus C is ['hello', ' world', ' my', ' large', ' language', ' model', ' is', ' coming', ' after', ' you', ' beware']\n",
            "Word frequences is {'hello': 1, ' world': 1, ' my': 1, ' large': 1, ' language': 1, ' model': 1, ' is': 1, ' coming': 1, ' after': 1, ' you': 1, ' beware': 1}\n",
            "{'hello': ['h', 'e', 'l', 'l', 'o'], ' world': ['_', 'w', 'o', 'r', 'l', 'd'], ' my': ['_', 'm', 'y'], ' large': ['_', 'l', 'a', 'r', 'g', 'e'], ' language': ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e'], ' model': ['_', 'm', 'o', 'd', 'e', 'l'], ' is': ['_', 'i', 's'], ' coming': ['_', 'c', 'o', 'm', 'i', 'n', 'g'], ' after': ['_', 'a', 'f', 't', 'e', 'r'], ' you': ['_', 'y', 'o', 'u'], ' beware': ['_', 'b', 'e', 'w', 'a', 'r', 'e']}\n",
            "Initial vocabulary is: ['_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'w', 'y']\n",
            "Pair frequencies:\n",
            "dict_items([(('h', 'e'), 1), (('e', 'l'), 2), (('l', 'l'), 1), (('l', 'o'), 1), (('_', 'w'), 1), (('w', 'o'), 1), (('o', 'r'), 1), (('r', 'l'), 1), (('l', 'd'), 1), (('_', 'm'), 2), (('m', 'y'), 1), (('_', 'l'), 2), (('l', 'a'), 2), (('a', 'r'), 2), (('r', 'g'), 1), (('g', 'e'), 2), (('a', 'n'), 1), (('n', 'g'), 2), (('g', 'u'), 1), (('u', 'a'), 1), (('a', 'g'), 1), (('m', 'o'), 1), (('o', 'd'), 1), (('d', 'e'), 1), (('_', 'i'), 1), (('i', 's'), 1), (('_', 'c'), 1), (('c', 'o'), 1), (('o', 'm'), 1), (('m', 'i'), 1), (('i', 'n'), 1), (('_', 'a'), 1), (('a', 'f'), 1), (('f', 't'), 1), (('t', 'e'), 1), (('e', 'r'), 1), (('_', 'y'), 1), (('y', 'o'), 1), (('o', 'u'), 1), (('_', 'b'), 1), (('b', 'e'), 1), (('e', 'w'), 1), (('w', 'a'), 1), (('r', 'e'), 1)])\n",
            "Most frequent pair to merge: ('e', 'l')\n",
            "Updated tokens per word:\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_', 'm', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_', 'm', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('h', 'el'), 1), (('el', 'l'), 1), (('l', 'o'), 1), (('_', 'w'), 1), (('w', 'o'), 1), (('o', 'r'), 1), (('r', 'l'), 1), (('l', 'd'), 1), (('_', 'm'), 2), (('m', 'y'), 1), (('_', 'l'), 2), (('l', 'a'), 2), (('a', 'r'), 2), (('r', 'g'), 1), (('g', 'e'), 2), (('a', 'n'), 1), (('n', 'g'), 2), (('g', 'u'), 1), (('u', 'a'), 1), (('a', 'g'), 1), (('m', 'o'), 1), (('o', 'd'), 1), (('d', 'el'), 1), (('_', 'i'), 1), (('i', 's'), 1), (('_', 'c'), 1), (('c', 'o'), 1), (('o', 'm'), 1), (('m', 'i'), 1), (('i', 'n'), 1), (('_', 'a'), 1), (('a', 'f'), 1), (('f', 't'), 1), (('t', 'e'), 1), (('e', 'r'), 1), (('_', 'y'), 1), (('y', 'o'), 1), (('o', 'u'), 1), (('_', 'b'), 1), (('b', 'e'), 1), (('e', 'w'), 1), (('w', 'a'), 1), (('r', 'e'), 1)])\n",
            "Most frequent pair to merge: ('_', 'm')\n",
            "Updated tokens per word:\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_', 'l', 'a', 'r', 'g', 'e']), (' language', ['_', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('h', 'el'), 1), (('el', 'l'), 1), (('l', 'o'), 1), (('_', 'w'), 1), (('w', 'o'), 1), (('o', 'r'), 1), (('r', 'l'), 1), (('l', 'd'), 1), (('_m', 'y'), 1), (('_', 'l'), 2), (('l', 'a'), 2), (('a', 'r'), 2), (('r', 'g'), 1), (('g', 'e'), 2), (('a', 'n'), 1), (('n', 'g'), 2), (('g', 'u'), 1), (('u', 'a'), 1), (('a', 'g'), 1), (('_m', 'o'), 1), (('o', 'd'), 1), (('d', 'el'), 1), (('_', 'i'), 1), (('i', 's'), 1), (('_', 'c'), 1), (('c', 'o'), 1), (('o', 'm'), 1), (('m', 'i'), 1), (('i', 'n'), 1), (('_', 'a'), 1), (('a', 'f'), 1), (('f', 't'), 1), (('t', 'e'), 1), (('e', 'r'), 1), (('_', 'y'), 1), (('y', 'o'), 1), (('o', 'u'), 1), (('_', 'b'), 1), (('b', 'e'), 1), (('e', 'w'), 1), (('w', 'a'), 1), (('r', 'e'), 1)])\n",
            "Most frequent pair to merge: ('_', 'l')\n",
            "Updated tokens per word:\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_l', 'a', 'r', 'g', 'e']), (' language', ['_l', 'a', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('h', 'el'), 1), (('el', 'l'), 1), (('l', 'o'), 1), (('_', 'w'), 1), (('w', 'o'), 1), (('o', 'r'), 1), (('r', 'l'), 1), (('l', 'd'), 1), (('_m', 'y'), 1), (('_l', 'a'), 2), (('a', 'r'), 2), (('r', 'g'), 1), (('g', 'e'), 2), (('a', 'n'), 1), (('n', 'g'), 2), (('g', 'u'), 1), (('u', 'a'), 1), (('a', 'g'), 1), (('_m', 'o'), 1), (('o', 'd'), 1), (('d', 'el'), 1), (('_', 'i'), 1), (('i', 's'), 1), (('_', 'c'), 1), (('c', 'o'), 1), (('o', 'm'), 1), (('m', 'i'), 1), (('i', 'n'), 1), (('_', 'a'), 1), (('a', 'f'), 1), (('f', 't'), 1), (('t', 'e'), 1), (('e', 'r'), 1), (('_', 'y'), 1), (('y', 'o'), 1), (('o', 'u'), 1), (('_', 'b'), 1), (('b', 'e'), 1), (('e', 'w'), 1), (('w', 'a'), 1), (('r', 'e'), 1)])\n",
            "Most frequent pair to merge: ('_l', 'a')\n",
            "Updated tokens per word:\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'g', 'e']), (' language', ['_la', 'n', 'g', 'u', 'a', 'g', 'e']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('h', 'el'), 1), (('el', 'l'), 1), (('l', 'o'), 1), (('_', 'w'), 1), (('w', 'o'), 1), (('o', 'r'), 1), (('r', 'l'), 1), (('l', 'd'), 1), (('_m', 'y'), 1), (('_la', 'r'), 1), (('r', 'g'), 1), (('g', 'e'), 2), (('_la', 'n'), 1), (('n', 'g'), 2), (('g', 'u'), 1), (('u', 'a'), 1), (('a', 'g'), 1), (('_m', 'o'), 1), (('o', 'd'), 1), (('d', 'el'), 1), (('_', 'i'), 1), (('i', 's'), 1), (('_', 'c'), 1), (('c', 'o'), 1), (('o', 'm'), 1), (('m', 'i'), 1), (('i', 'n'), 1), (('_', 'a'), 1), (('a', 'f'), 1), (('f', 't'), 1), (('t', 'e'), 1), (('e', 'r'), 1), (('_', 'y'), 1), (('y', 'o'), 1), (('o', 'u'), 1), (('_', 'b'), 1), (('b', 'e'), 1), (('e', 'w'), 1), (('w', 'a'), 1), (('a', 'r'), 1), (('r', 'e'), 1)])\n",
            "Most frequent pair to merge: ('g', 'e')\n",
            "Updated tokens per word:\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'n', 'g', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'n', 'g']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "Pair frequencies:\n",
            "dict_items([(('h', 'el'), 1), (('el', 'l'), 1), (('l', 'o'), 1), (('_', 'w'), 1), (('w', 'o'), 1), (('o', 'r'), 1), (('r', 'l'), 1), (('l', 'd'), 1), (('_m', 'y'), 1), (('_la', 'r'), 1), (('r', 'ge'), 1), (('_la', 'n'), 1), (('n', 'g'), 2), (('g', 'u'), 1), (('u', 'a'), 1), (('a', 'ge'), 1), (('_m', 'o'), 1), (('o', 'd'), 1), (('d', 'el'), 1), (('_', 'i'), 1), (('i', 's'), 1), (('_', 'c'), 1), (('c', 'o'), 1), (('o', 'm'), 1), (('m', 'i'), 1), (('i', 'n'), 1), (('_', 'a'), 1), (('a', 'f'), 1), (('f', 't'), 1), (('t', 'e'), 1), (('e', 'r'), 1), (('_', 'y'), 1), (('y', 'o'), 1), (('o', 'u'), 1), (('_', 'b'), 1), (('b', 'e'), 1), (('e', 'w'), 1), (('w', 'a'), 1), (('a', 'r'), 1), (('r', 'e'), 1)])\n",
            "Most frequent pair to merge: ('n', 'g')\n",
            "Updated tokens per word:\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "dict_items([('hello', ['h', 'el', 'l', 'o']), (' world', ['_', 'w', 'o', 'r', 'l', 'd']), (' my', ['_m', 'y']), (' large', ['_la', 'r', 'ge']), (' language', ['_la', 'ng', 'u', 'a', 'ge']), (' model', ['_m', 'o', 'd', 'el']), (' is', ['_', 'i', 's']), (' coming', ['_', 'c', 'o', 'm', 'i', 'ng']), (' after', ['_', 'a', 'f', 't', 'e', 'r']), (' you', ['_', 'y', 'o', 'u']), (' beware', ['_', 'b', 'e', 'w', 'a', 'r', 'e'])])\n",
            "\n",
            "final vocabulary after all merges:\n",
            "['_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'w', 'y', 'el', '_m', '_l', '_la', 'ge', 'ng']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "corpus = \"set new new renew reset renew\"\n",
        "assert set(bpe_train(corpus, 4)) == set(['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new', '_r', '_re'])\n",
        "assert set(bpe_train(corpus, 8)) == set(['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new', '_r', '_re', 'se', 'set', '_new', '_renew'])\n",
        "assert set(bpe_train(corpus, 9)) == set(['_', 'e', 'n', 'r', 's', 't', 'w', 'ne', 'new', '_r', '_re', 'se', 'set', '_new', '_renew', '_reset']);_\n",
        "corpus = \"hello world my large language model is coming after you beware\"\n",
        "assert set(bpe_train(corpus, 6)) == set(['_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'w', 'y', 'el', '_m', '_l', '_la', 'ge', 'ng'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Exercise1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
