{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akprodromou/Natural-Language-Processing/blob/main/exercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dUVx3m8uudq"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zR1R-5rfuudr"
      },
      "outputs": [],
      "source": [
        "NAME = \"ANTONIS PRODROMOU\"\n",
        "AEM = \"238\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K_wdrvjuuds"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QBYMe9msl-M"
      },
      "source": [
        "# Introduction and learning goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LrM8wlcsbFl"
      },
      "source": [
        "Welcome to your 3rd assignment! The goal is to get hands-on experience with training feed-forward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wry00RvoWrg8"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dudWV1tVWxWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38278e48-ce8b-4353-bae8-715732d9fc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version: 3.10.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# Setting random seeds for reproducible grading\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"Keras Version:\", keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6vjzI8aF0eW"
      },
      "source": [
        "# Reading and preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4N1OoaG8eN"
      },
      "source": [
        "Load the IMDb dataset using keras.datasets.imdb.load_data. Only keep the top 10,000 most frequent words (num_words=10000). Store the result to variables x_train, x_test, while the associated ground truth data should be stored to y_train and y_test respectively.\n",
        "\n",
        "These data are already tokenized into lists of numbers correspondings to tokens. Therefore the next step is to pad/truncate them to a maximum length using keras.preprocessing.sequence.pad_sequences. Set this length to 256. Store the updated data to the same x_train and x_test variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": false,
        "id": "XsjEuYHUFzjl",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "06648f6a63256e6b0b3a610abc2b9615",
          "grade": false,
          "grade_id": "cell-962027ab79fbb9a2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "keras.utils.set_random_seed(42)\n",
        "\n",
        "# load the IMDb dataset\n",
        "# only keep the 10,000 most frequent words (this becomes the size of our vocabulary)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n",
        "# padding and truncation are strategies for creating rectangular tensors from batches of varying lengths\n",
        "# pad/truncate (x_train, y_train) to a 256 maximum length\n",
        "# store the updated data to the same x_train and x_test variables\n",
        "x_train = tf.keras.utils.pad_sequences(\n",
        "    x_train,\n",
        "    maxlen=256,\n",
        "    dtype='int32',\n",
        "    padding='pre',\n",
        "    truncating='pre',\n",
        "    value=0.0\n",
        ")\n",
        "\n",
        "x_test = tf.keras.utils.pad_sequences(\n",
        "    x_test,\n",
        "    maxlen=256,\n",
        "    dtype='int32',\n",
        "    padding='pre',\n",
        "    truncating='pre',\n",
        "    value=0.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "32KDFlt5F-c0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "92e8f78acff737cc344ba86a41c81cc7",
          "grade": true,
          "grade_id": "cell-f1a7a182be73d6f1",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert x_train.shape == (25000, 256), f\"Expected x_train shape (25000, 250), but got {x_train.shape}\"\n",
        "assert x_test.shape == (25000, 256), f\"Expected x_test shape (25000, 250), but got {x_test.shape}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MBAQLt3KUz2"
      },
      "source": [
        "# Creating the architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OV25jwdH3NW"
      },
      "source": [
        "Create the neural architecture. You should use an embedding layer with 32 dimensions for each word. Your architecture should concatenate these embeddings. Then you should use a dense layer with 64 nodes and a ReLU as activation function. Finally you should use an appropriate output layer. Store the model in a variable called model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "id": "XNBRFUP0I20P",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e72817c4ab45630289334c07d3c82a0",
          "grade": false,
          "grade_id": "cell-263f59099c414f3c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# initialize the model\n",
        "# store the model in a variable called model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# embedding layer with 32 dimensions for each word\n",
        "model.add(keras.layers.Embedding(\n",
        "    # size of vocabulary\n",
        "    input_dim = 10000,\n",
        "    # embedding length\n",
        "    output_dim = 32\n",
        "))\n",
        "\n",
        "# concatenate these embeddings for each review\n",
        "# flatten does not include the batch dimension\n",
        "model.add(keras.layers.Flatten(\n",
        "    # input_shape=(256, 32)\n",
        "))\n",
        "\n",
        "# use a dense layer with 64 nodes and a ReLU as activation function\n",
        "model.add(keras.layers.Dense(\n",
        "    units = 64,\n",
        "    activation='relu'\n",
        "))\n",
        "\n",
        "# use an appropriate output layer\n",
        "# since our classification is binary, we'll use sigmoid\n",
        "model.add(keras.layers.Dense(\n",
        "    units = 1,\n",
        "    activation='sigmoid'\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-94ms2yRJVEJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e30bd347aa34992d01fde1be6f802860",
          "grade": true,
          "grade_id": "cell-9cbbc4852212fb62",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert len(model.layers) == 4, \"Model should have exactly 4 layers\"\n",
        "assert isinstance(model.layers[0], keras.layers.Embedding), \"First layer must be Embedding\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJI0SfXdJ_RH"
      },
      "source": [
        "Compile the model using the appropriate loss function. Study the available loss functions here: https://www.tensorflow.org/api_docs/python/tf/keras/losses . Which one should you use given the representation of the target values in our dataset and your choice of output layer? The following code prints the target values for the first 20 training examples. Use the Adam optimizer with a learning rate of 0.0001. Use accuracy as metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BzYBuV-cugjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371ba7d7-5884-41ea-fa96-5ae5bd3de477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_train[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "deletable": false,
        "id": "pEinKhbaJ_Yj",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "538596a319b184940eb510be271b09d4",
          "grade": false,
          "grade_id": "cell-93aff1e9854cbfc5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# compile is a model method\n",
        "model.compile(\n",
        "    # Adam optimizer with a learning rate of 0.0001\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    # cross entropy for binary classification\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[\n",
        "        # use accuracy as metric\n",
        "        'accuracy'\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oPOGQ100u38N",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "66dce571301effccdc50308fa93228ff",
          "grade": true,
          "grade_id": "cell-dd8d5978df307d9d",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert model.optimizer.learning_rate == 0.0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIfXQzy5yZt4"
      },
      "source": [
        "# Training and evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V25r3e8cuEdb"
      },
      "source": [
        "Fit the model for 5 epochs, using 1/4 of the data as validation set, and presenting the metrics also for the validation set at the end of each epoch. Save the fit results to a history variable. Use a batch size of 128 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "id": "lt64cQ-puBtG",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "31deba08daa5674031b350fb9a70aa86",
          "grade": false,
          "grade_id": "cell-8bc10fdeb6524402",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55032f12-d703-4091-964a-f99f17d1994e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.5192 - loss: 0.6920 - val_accuracy: 0.5453 - val_loss: 0.6887\n",
            "Epoch 2/5\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6695 - loss: 0.6735 - val_accuracy: 0.7045 - val_loss: 0.6337\n",
            "Epoch 3/5\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.7772 - loss: 0.5724 - val_accuracy: 0.8139 - val_loss: 0.4642\n",
            "Epoch 4/5\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.8594 - loss: 0.3915 - val_accuracy: 0.8526 - val_loss: 0.3675\n",
            "Epoch 5/5\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.8936 - loss: 0.2943 - val_accuracy: 0.8634 - val_loss: 0.3273\n",
            "Accuracy History per epoch: [0.5271999835968018, 0.6822400093078613, 0.8036800026893616, 0.8692799806594849, 0.8993066549301147]\n"
          ]
        }
      ],
      "source": [
        "# Reserve 1/4 of the data for validation, i.e. 6250\n",
        "x_val = x_train[:6250]\n",
        "y_val = y_train[:6250]\n",
        "x_train = x_train[6250:]\n",
        "y_train = y_train[6250:]\n",
        "\n",
        "# fit() will train the model by slicing the data into \"batches\" of size 'batch_size',\n",
        "# and repeatedly iterating over the entire dataset for a given number of epochs\n",
        "# save the fit results to a history variable\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    # use a batch size of 128 samples\n",
        "    batch_size=128,\n",
        "    # fit the model for 5 epochs\n",
        "    epochs=5,\n",
        "    # validation for monitoring validation loss and\n",
        "    # metrics at the end of each epoch\n",
        "    validation_data=(x_val, y_val),\n",
        ")\n",
        "\n",
        "print(f\"Accuracy History per epoch: {history.history['accuracy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "jSQHc3etWcQQ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e7b645e472d0956ccaf392283b5641f",
          "grade": true,
          "grade_id": "cell-7c3adbfbdc861614",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "final_accuracy = history.history['accuracy'][-1]\n",
        "assert final_accuracy > 0.85"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OksXOL1r0iT0"
      },
      "source": [
        "Evaluate the model on the test data and store the accuracy in variable called accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "id": "qHCbGws3hPor",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "23be58794cab96685d73fd423dd68b87",
          "grade": false,
          "grade_id": "cell-6e1b0a3afc2ef2e8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45748637-00e4-4e85-a6fd-7d986e6e3b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8629 - loss: 0.3286\n",
            "test loss, test acc: [0.3284449875354767, 0.8626000285148621]\n",
            "accuracy: 0.8626000285148621\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluate on test data\")\n",
        "test_metrics = model.evaluate(x_test, y_test)\n",
        "print(\"test loss, test acc:\", test_metrics)\n",
        "accuracy = test_metrics[1]\n",
        "print(\"accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Oyma9RkQ1XMH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "769a454c910b33ef1ba3fd8d6df9ffd1",
          "grade": true,
          "grade_id": "cell-ea176695385c9643",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert accuracy > 0.85"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}